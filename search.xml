<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Spark Streaming Back pressure Internals]]></title>
    <url>%2F2019%2F02%2F19%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Receiver介绍Spark Streaming中负责数据接收的是Receiver，在Streaming作业开始的时候，通过SparkStreamingContext调用， Receiver的作用是接收数据以及存储数据。然后Spark中负责数据处理的模块将存储在Spark的BlockManager中的通过创建新的RDD数据转化为DStream。之后用户就可以通过Spark Streaming中的两种操作：transformation和output operation来操作DStream。对于每个输入的数据源，只会有一个Receiver。 Spark Streaming中的数据流在Spark Streaming中，当一个Receiver启动的时候，在每个spark.streaming.blockInterval所设定的毫秒时间间隔中，都会新建一个block，每个block会成为DStream中的RDD的一个Partition。用户的输入源可能有多种多样，比如KafkaInputDStream，这些不同的DStream来源，如何确定其Partition的数目呢？其实是通过batchInterval / spark.streaming.blockInterval 来确定的，而batchInterval就是用户在创建SparkStreamingContext时传递给构造器的时间参数，举例来说，如果一个batch的Interval是2s(默认情况)同时blockInterval的间隔是200ms(默认情况)，那么RDD将会有10个partition。一旦RDD创建成功，其处理过程可以像一个job一样被driver的JobScheduler调度，在当前的实现中，Spark Streaming如果采用默认的配置，任意一个时间点上只有一个job是active的，所以如果一个batch的处理时间比interval长，那么下一个batch的job将会被阻塞。job的并发数只有1的原因是，多个job并发执行可能会导致复杂的资源共享问题，在资源不足的时候，这些问题很难debug。job并发数为1的话则会比较容易发现：只要job的处理速度比interval间隔短，那么系统就是稳定的。数据和元数据的情况如下所示： RDD创建之前的数据流用粗箭头表示，元数据使用正常宽度的箭头表示。private的数据结构使用虚线框表示，public的数据结构使用实现框表示，队列使用红色表示。一些要点： ReceiverSupervisor在每个DStream中只处理一个Receiver 在ReceivedBlockHandler和Block生成的过程中的WriteAheadLogManager使用相同的结构 如图所示，BlockManager以及ReceiverTracker使用actor来通信以及传输元数据 一旦作业提交，streaming程序的数据源的个数就是确定的，因为reciever是通过streamingContext.start()方法启动的，所以必须在context启动之前把所有的DStream的输入创建好，只要context完成实例化，那么任何DStream输入数据源都没办法继续创建了最后，SPARK-1341这个PR只在Spark将block切片的时候起作用，所以，如果blocks是被迭代器传输过来的，那么节流的方式就没办法起作用了。 难点前面描述的Spark中RDD的block的创建过程中，数据会先加入到Spark的cache中，然后在从Spark的cache或者checkpoint中移除，问题在于Spark cache中的数据的移除速度并不是和加入的速度一样快的，如果存储这些block使用内存的话，很可能狐疑引发OOM。在block的整个生命周期中，有三处是可能会因为一些原因而需要更多的 时间的： block复制过程变慢 WAL写过程变慢 处理job过程变慢最最常见的情况是第三种。大多数的Spark作业调优的方式都是通过调整参数来达成以下的目标：batch处理时间 &lt; batch间隔 Batch的处理时间由于输入数据的多少而差别很大，所以相同的时间间隔内，如果用户数据产生的比较多，那么会比相对不那么活跃的时间段，Streaming程序需要处理的数据量会少很多。 如果想让系统的吞吐量更大，那么应该让配置更契合资源情况，而不是在batch间隔上调来调去。 对于任何一个给定的Streaming的配置来说，如果输入数据爆增，那么程序还是会挂掉。所以，系统的目标应该是： 在数据处理变慢的时候通过信号量通知数据产生的模块(这里是ReceiverSupervisor) 在Reciever所在的Executor通过模块化和可配置参数实现一种可以优雅控制拥塞的策略（比如可以对暴增的数据不消费、丢弃、采样处理等）总体目标是在数据量变化的同时保证Spark Streaming系统的稳定性。 这部分描述解决方案 Back-pressure 信号量目标是为了JobScheduler可以管理queue的长度同时根据作业的长度来请求接受新的数据。为了达成这个目标，我们从下面的一个标准得到了灵感：Reactive Streams是首创的为异步流处理提供标准的非阻塞式back-pressure机制的。它包含一份约定（包含4个级别以及大概30个规则）来达成高吞吐量以及弹性。这份约定描述了Publisher和Subscriber之间的安全且高效的通信规则。尽管我们会从Reactive Streams的规范中得到了灵感，但是我们不打算在Spark内部实现这个规范。不过，以下是一些可以吸取的地方： - 这个规范整体是建立在下游消费端的buffer大小是固定的情况下。规范中，消费端的Subscriber发送一个back-pressure信号量来反馈当前的阻塞情况 - Subscriber端可接受的对象的数目是有限的。但是在这个界限内，Publisher可以以最快的速度发送消息。这样其实就实现了这样一种机制：“Reactive Streams在消费端比发送端快的时候是push方式的，在发送端比消费端快的时候是pull方式的” - 这个信号量异步的，所以Reactive Streams的API其实规定了所有的消费者的back-pressure的信号量其实都是无阻塞的。然而，Publisher传输数据的方式可以是异步的也可以是同步的 复用性Back-pressure信号量是暴露出来的，所以： - 纵向的，下推stream(比如根据job scheduler的信号做block复制) - 横向的，stream不同位置不一样。比如，BlockGenerator可以将它的动作根据信号量裁剪，比如WAL以及block复制。 特别的，一个Stream是可以有一种“扇出”(fan-out)的配置，数据或者经过map等操作，产生新的版本的时候，并行的流入两个新的位置。Reactive Stream支持这种操作，同时为了衡量block 复制和WAL，我们在这个章节的结尾会讨论这个，我们会实现一种。更多的细节请参考：https://doc.akka.io/docs/akka-stream-and-http-experimental/1.0-M5/scala/stream-graphs.html 兼容性：如何发送back-pressure信号Spark Streaming在每个batch中只产生一个batch的job，在每个blockInterval只产生一个block。这些限制是固有的，不会被改变，即使仅仅是为了后向兼容。所以，减轻处理压力的最好方式是在每个interval中减少记录的条数。这意味着我们的可调参数其实是每个block中的record数目。回顾下之前我们提出的为了解决数据拥塞的策略： - 暂停（比如在block间隔中加开关） - 丢弃数据 - 采样 这些策略其实都可以使用在这个场景中-这些策略和额固定batch time产生固定的一个batch job以及固定的batch interval产生一个block，这意味着我们的实现不去参考Reactive Stream那一套规范。比如，规范1.1说：整体上，Publisher发给Subscriber的onNext信号的总数必须小于等于Subscriber的总共的record数目。但是，我们实现的是：整体上，BlockGenerator产生的每个block的record数目必须小于等于subcribor的总共的record数目。 解决方案1. 增加一种back-pressure机制。这种机制会在每个block生成的时候为每其加上一个record数目的上限，这个限制可以是强制的也可以不是，取决于配置，这个机制将JobScheduler中的jobSet中将每个job的record数目保存在BlockGenertator中。 2. 基于back-pressure提供一种可以在BlockGenerator使用的API a. 默认实现将会将会采用当前的机制（没有back-pressure） b. 一个新的选项。从信号中读取record知道block interval时间到了或者需要的record数目限制达到了 3. 当record数目的上限比收到的数据多时，提供三种等价的策略来实现输入数据管理 a. 丢弃数据 b. 随机采样 c. Receiver采用和Reactive Strem中的Subscriber一样的接口实现 4. 在BlockManager将block信息发送到BlockGenerator的过程中增加back-pressure信息，然后将数据复制的速度暴露给后续的其他数据源 5. 在WAL机制写入到BlockGenerator的过程中增加back-pressure信息，将这个信息暴露给上述所有的数据源， 目标1和2以及2是这个章节主要讲述的，4和5会在接下来讲述 要求数据恢复这些back-pressure相关的机制不能影响现有的Spark Streeaming的容错；Spark Streaming的容错机制其实只是创建新的block，这个步骤现在并不是幂等的。只是修改Block的创建，并不会更改已有的容错性机制。更进一步，WAL机制应该在注册block之前激活，这时会阻塞HDFS的写操作。这点和Reactive Stream不同。最后，因为丢弃数据的策略其实是有损耗的，会影响结果的正确性或者精度，诸如丢弃数据或者对数据采样的方式永远不应该成为默认配置。 性能Back pressure的异步信号量是为了搞吞吐量而设计的。然而，基于Spark Streaming的这项工作应该做一下性能测试来确保这个心加入的机制没有影响原先的系统性能、 易维护性其实容易维护一定程度上也应该采用实现起来较为简单的方式，而且测量是否发生拥塞的模块也应该在local级别，这样程序易读性也会更好。特别要指出的是，不同批次的request信号应该被认真区别，这样一来，想要接受指定数量的record就比较简单了。 可选方案Spark Streaming一个可替代方案是在架构的不同点上对负载引入速率限制，就像SPARK-1341的解决方案一样。然而，这会给系统引入更多的队列，不仅不能保证在负载下的稳定运行，而且会增加设置的数量，这样需要更多的调整来做保证稳定性。另外一个可选的方案是使用back-pressure阻塞信号，但是这样在资源充足，延迟较低的时候回降低性能。另一种方法时基于信号中record所占用的内存大小， 一方面是相关的executor上的可用内存，另一方面是 - 某一时间内的cache的内存对task来说是不可用的：因为Receiver满载了，占用了executor的所有内存 - 这种方法比较难以实现，因为内存并不仅仅是给需要读取的数据来做缓存的，同时还有中间计算产生的临时RDD - 对于若干个DStream是否拥塞的判断方法不是很清楚：Receiver所在的Executor除了接受源数据也会接受其他DStream复制来的block - 最后，从信号中读取记录对内存的影响很难做到百分百精确，即比基于缓存中适时的快照好 然而，一个显而易见的好处是这种中心化的内存管理方式保证了不会出现OOM。这样说来，OOM可能发生的场景非常有限了，我们对于精度的要求也就可以适当的降低了。需要注意的是，这里仍然有两种可能发生OOM的场景： - 如果不同的batch interval之中的记录数目差距很大，比如我们现在有两个Dstream[Array[Int]], 前者包含的数据比较少，而后者包含的数据比较多，那么当back-pressure是在前者测量的，那么在后者的数据比较多的情况下，可能会导致此时发生了错误的估计，从而导致OOM - 每个batch的处理时间是根据之前处理的batch的时间然后线性计算的，如果计算出现差错，那么可能会导致OOM 实现细节Reactive Streams是传输无关的，意味着Publisher和Subscriber之间(这里特指Driver上的JobScheduler和executor上的Receiver)不存在传输协议我们会复用Spark Streaming中的actos来传输这些消息。这样我们只需要在ReceiverTracker(运行在Jobscheduler中)和ReceiverSupervisor()中增加一条信息。实现中，JobScheduler发送的record数目和每个batch interval中被清理掉的record数目是一样的。为了防止job队列积压，每个作业都会在batch结束的时候发送自己的scheduling delay时间。这样，我们可以在最后一个batch中按照比例减少这个batch中的record，以保证尽可能的降低scheduling delay。这部分的工作可以用一个PID 控制器模块来完成。然后Back-Pressure信号的接受者：BlockGenerator会调整下面这个参数：每个block容纳的record个数上限来适配record个数和处理时间，以使得最后一个batch interval中的elements数目是调整到最佳的需要指出的地方是，back-pressure异步发送的关于每个batch interval中的record数目的信号是通过在每个batch interval中采样得到的： 1. JobScheduler测量最后一个batch interval的合理间隔的时候，依赖于batch和block interval，但并不是简单的计算一下每个batch的数据量，比如某个batch的作业是一个慢作业，从第n个batch开始运行，到第n+2个batch才结束，那么下一个batch interval就是这个job接受的record个数除以2 2. 如果JobScheduer发现Job中一个Job的处理能力比当前实际处理的数据大，比如当前的第n个batch被处理的时间只有batch interval的一半，那么下次传输来的block中的record个数就会是当前的两倍 3. 如果最近的batch中的record个数为0，那么数据信号量就会使用Int.maxValue来作为下一次需要的数据记录的请求个数，也就是对每个block中的数据记录的数量不做限制 这也意味着：在一个batch中，处理时间的长短和block中的数据量多少是线性相关的，我们假设处理时间可以根据处理的数据量来做线性调整，比如根据当前时间调整大一些或者调整小一些，当然，这种假设在处理时间和处理数据量之间的关系是非线性的时候是不成立的onNext方法会通知BlockGenerator生成自定义record数量的block，这个数量是back-pressure信号的值决定 代码修改当前架构 数据到达receiver，然后在BlockManager中注册，同时被复制到其他BlockManager Receciver tracker收到block Id同时维护一个batch时间和已经分配的block Id的映射表，以及一个未分配的block id的队列 JobGenerator在每个batchInterval间隔中得到一个事件，这个事件触发JobGenerator将未分配的block 的Id在下一个新的batch中分配，同时生成一个处理这些block的JobSet JobScheduler调度这个JobSet 修改后的架构 修改之后的流程： 增加新组件RateController，这个新组件监听onBatchCompleted消息，然后根据processingDelay和schedulingDelay估计处理速度然后发送。这个组件的主要用途是估算每个流的最大处理速率(记录数/时间)。估算使用的算法以后可能会有不同实现，现在使用的PID控制算法 InputDStreams维护一个保存最大速率的变量，用来接收RateController预估的速度 默认情况下，Receiver-based 方式将这个值(条目2中所说的速率)通过ReceiverTracker和ReceiverSupervisor发送给BlockGenerator(back-pressure开启的情况下其实是RateLimiter) 其他的Dstream(kafka receiver based之外的)需要在各自内部实现这种机制,由于Streaming程序会面临冷启动以及恢复重启等场景，所以之前的速率控制参数spark.streaming.kafka.maxRatePerPartition以及spark.streaming.receiver.maxRate可以保留来并在这些场景下使用 实现细节 在InputDStream中添加RateController来监听消息总线同时为自己的这DStream估算速率，这个操作是RateEstimator完成的 RateEstimator接口定义了一个方法 123456def compute( time: Long, //BatchInfo.processingEndTime elements: Long, // elements in this batch processingDelay: Long, // BatchInfo.schedulingDelay schedulingDelay: Option[Long] // BatchInfo.schedulingDelay): Option[Long] // maximum estimated rate (elements/second) 实现方式中会采用batch interval作为构造参数 增加RateController的两种实现方式，一个是为Receiver-based ReceiverInputDStream，一个是为DirectKafkaInputDStream准备的。ReceiverRateController通过receiver tracker将速度信号异步发送给receiver，然后kafka这边使用这个参数计算下一个partition的大小。 RateEstimator的一个实现： PIDRateEstimator，此处实现了PID控制算法，误差信号是输入的速率和处理的速率(计量单位是record数目每秒),实现细节请参看Operating on back-pressure in Spark with a PID一文，比例，微分，积分三个参数都是可以配置的，默认情况其实是相当于PI(比例微分)控制(积分项的系数是0)。 增加了一个新的配置项，spark.streaming.rateEstimator，来配置使用哪一种RateEstimator的实现，可以是noop或者PID ReceiverRateController 细节增加新的消息：RateUpdate(newRate: Long)这样可以在ReceiverSupervisor中控制速率增加新的方法：ReceiverTracker中增加sendRateUpdate(streamId: Int, newRate: Long): Unit，这个方法根据streamId异步发送RateUpdate消息给ReceiverSupervisor控制流： 注意：ReceiverRateController属于ReceiverInputDStream，接受streaming中的消息总线的batch结束的的信号，然后如果这些信号中有和这个stream相关的，就调用RateEstimator中的compute()函数来更新更佳的目标输入速率值ReceiverRateController通过调用ssc.scheduler.receiverTracker.sendRateUpdate(id, newRate)方法Spark Streaming中的InputDStream实现ReceiverInputDStream 通过ReceiverSupervisor将这个速率传送到BlockGenerator，然后BlockGenerator根据这个新的值更新其速率限制FileInputDStream 这个比较难，因为不完整的读取文件的话，这里没有办法得知文件中有多少记录QueueInputDStream 这个和FileInputDStream类似RawInputDStream 原始字节流难以通过 记录数/时间 计算速率SocketInputDStream 不需要修改，Receiver-based机制会处理速率限制Spark Streaming的外部实现DirectKafkaInputDStream 从其自身的maxRatePerPartition切换到更high level的速度限制方法KafkaInputDStream Receiver-based的DStream将会从back-poressure机制中直接受益，但是 如果使用ReliableKafkaReceiver(useReliableReceiver == true)，这里使用定制的BlockGenerator。我们需要在实现中传输速率FlumeInputDStream 不需要修改，receiver-based机制会处理速率限制FlumePollingInputDStream 这个DStream按照batch来推送数据，这样会短路BlockGeneratorMQTTInputDStream 不需要修改，receiver-based机制会处理速率限制TwitterInputDStream 不需要修改，receiver-based机制会处理速率限制 参考： Reactive Stream 介绍 JDK9揭秘：Reactive Streams Spark Streaming Back-Pressure Design Doc1 Spark Streaming Back-Pressure Design Doc 2 Spark Streaming back pressure Spark Streaming Backpressure - finding the optimal rate is now done automatically Operating on back-pressure in Spark with a PID]]></content>
  </entry>
</search>
